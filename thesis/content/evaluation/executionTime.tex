\section{Execution Time}
\label{sec:eval_executionTime}
The second aspect we want to evaluate is the execution time of different compilation stages. Since our compiler does not implement the lexical and syntactic analysis itself but uses ANTLR to generate the lexer and parser, our evaluation focuses on the semantic analysis, code generation, and, finally, optimization. To analyze the performance, we compiled the program with the ripple-carry adder with an input of $a = \frac{1}{\sqrt{2}} (\ket{0} + \ket{3})$ and $b = \ket{15}$. In this example, the complexity of the compiler circuit can be adjusted by changing the register sizes for $a$ and $b$. For our evaluation, we used the powers of two from $16$ to $1024$. For each register size $n$, we compiled the program ten times and took the averages of the execution times for each compilation stage. The results are depicted in Fig.~\ref{tab:eval_executionTime}. 

\begin{table}[htp]
    \centering     
    \begin{tabular}{c|ccc}
    \multirow{2}{*}{Register Size $n$} & \multicolumn{3}{c}{Execution Time of Stages in ms}                                                  \\ \cline{2-4} 
                                       & \multicolumn{1}{c|}{Semantic Analysis} & \multicolumn{1}{c|}{Code Generation} & Optimization \\ \hline
                                       16                                 & \multicolumn{1}{c|}{28.3}                & \multicolumn{1}{c|}{45.4}              & 117          \\
    32                                 & \multicolumn{1}{c|}{27.5}                & \multicolumn{1}{c|}{43.8}              & 215.4          \\
    64                                 & \multicolumn{1}{c|}{27.3}                & \multicolumn{1}{c|}{47.8}              & 711.6          \\
    128                                & \multicolumn{1}{c|}{26.3}                & \multicolumn{1}{c|}{50.4}              & 2292.4         \\
    256                                & \multicolumn{1}{c|}{26.2}                & \multicolumn{1}{c|}{59.7}              & 10755.7        \\
    512                                & \multicolumn{1}{c|}{25.8}                & \multicolumn{1}{c|}{74.9}              & 60204.7        \\
    1024                               & \multicolumn{1}{c|}{26.1}                & \multicolumn{1}{c|}{109.1}             & 405376.6      
\end{tabular}
\caption{The execution times compiling a quantum ripple-carry adder with different register sizes.}
\label{tab:eval_executionTime}
\end{table}

The first stage is the semantic analysis. In this stage, the source code is analyzed for semantic errors, \eg undeclared variables or type errors. Since the analysis is performed on the source code, the execution time does not change significantly with a change in register size, as the source code size remains unchanged. The execution times range from $25.8$ ms to $28.3$ ms with an average of $26.79$ ms. 

Next, the code generation stage generates the target code from the source code. In our example program, the source code size stays constant. Both the \texttt{MAJ} and \texttt{UMA} gates are not dependent on the register size. However, the adder gate iterates over the size of the registers with a loop statement twice. Since the loop statements are unrolled at compile time, the generated code size increases linearly with an increase in register size, and, in turn, the execution time should increase likewise. This trend is not clear for small register sizes between $16$ and $64$. However, for larger register sizes, a linear trend is detectable.

Lastly, the optimization stage applies optimization rules, if possible, to the generated quantum circuit and, thereby, reduces its complexity. The optimization algorithm iterates over the circuit by going through all qubits. From there, it iterates over all gates on the qubit wire. In the case that all gates operate on all qubits, the resulting circuit iteration has a complexity of $n_g \cdot n_q$ where $n_g$ and $n_q$ are the number of gates and qubits, respectively. Additionally, the circuit iteration is repeated as often as optimizations can be found. In the case that all gates can be optimized and are only optimized one gate at a time, the overall complexity of the optimization is $n_g^2 \cdot n_q$. Since the number of gates increases linearly with the register size in our example, the worst-case complexity for its optimization is $\mathcal{O}(n^3)$, where $n$ is the register size. While the worst case would be a cubic increase in execution time when increasing the register size, the data shows an approximate quadratic increase, starting at $117$ ms on average for a register size of $16$ and going up to about $6$ minutes and 45 seconds for a register size of $1024$.

