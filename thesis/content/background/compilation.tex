\section{Compilation}
\label{sec:background_compilation}
The execution on a computer is controlled by a program. This program is written in a specific language unique to the hardware of the computer, machine code. However, this language is often neither human readable nor suitable for writing complex systems. Therefore, most programs are written in a more accessible language. The program can then be translated to the machine code with a \emph{compiler}. 

A compiler translates a program written in a source language to a program in a target language. The compilation process can be divided into multiple steps. The first step is the \emph{lexical analysis} to transform the source code into a sequence of tokens. Next, the syntactic structure of the code is analyzed be the \emph{parser}. Then, the code is \emph{semantically analyzed} to find semantic errors and infer information for the following phases. Lastly, the \emph{code generation} step generates the code in the target language. Additionally, the compiler may perform optimizations on the code before generating the target code or it may \emph{optimize} the resulting target code~\cite{Oliv07,VSSD07}. In the following, we be discuss the different steps of a compiler individually.

\subsection{Lexer (Lexical Analysis)}
\label{sec:background_lexer}
The lexical analysis of the source program takes the character stream and groups together associated characters producing a sequence of tokens~\cite{Oliv07}. Therefore, the step is also referred to as \emph{tokenization}~\cite{Gref99}. The process can be divided into the \emph{scanning} and \emph{screening} of the character and token sequence~\cite{DeRe74}.

The scanning process groups together substrings into textual elements, or tokens. In contrast to the characters and substrings, these tokens have defined meanings and may have additional attributes. For example, they may include identifiers, operator, comments, and spaces. In the case of the identifier token, an additional attribute could be the string value of the identifier. They can be specified with the help of a regular grammar or regular expression~\cite{DeRe74,VSSD07}.  

After being divided into a sequence of tokens, the screening step drops any characters or sequences of characters not relevant to the compilation from the program code. 
These may include characters such as spaces and tabs, or white space in general, and character sequences such as comments. 
Further, is may also recognize additional special symbols, such as keywords, and map them to a designated token. For example, a identifier with a value of ``while'' could be mapped to the corresponding token of the \texttt{while}-toke.\cite{DeRe74}.

Some example regular expressions for a lexical analysis are depicted in Fig.~\ref{fig:example_lexer}. The code depicts regular expressions for integers, identifiers, comments, and white space in ANTLR syntax.
The integer can either be an arbitrary sequence of characters between zero and nine without a leading zero or just zero with a length of at least one. Similarly, an identifier is a sequence of lower and upper case alphabetical characters, numbers, and underscores with a length of at least 1 and without a leading number. In contrast, a comment is any string starting with a double slash until the line break and white space is any white space characters. Additionally, the comment and white space also define a scanning step where both are discarded.

\begin{figure}[htp]
    \centering
    \lstinputlisting[style=ANTLR]{../figures/code/background/example_lexer.g4}
    \caption{An example of a regular grammar for the lexical analysis.}
    \label{fig:example_lexer}
\end{figure}

\subsection{Parser (Syntax Analysis)}
\label{sec:background_parser}
The lexical analysis of the compiler yields a sequence of tokens with a known meaning; the structure of the program, however, is not apparent in the token sequence. For example, an operator-token does not indicate what the operands are. To gain knowledge of the structure of the program, the parser step analyzes the syntactic structure of the source program and creates a parse tree from it. The compiler can then use the tree by, \eg, walking over it to generate the target code. This step should also detect and report any syntactical errors, like a missing closing parentheses~\cite{VSSD07}.

While the lexical analysis can be achieved with regular expressions, the syntactic structure of a program must be represented by, at least, a context-free grammar. Since regular languages are a subset of context-free languages, 
%\unsure{citation needed? (Chomsky, "Three models for the description of language")}
the parsing step can also perform the lexical analysis. However, there are multiple reasons why the lexical and syntax analysis are separated. Firstly, the separation of both analysis makes the compiler more modular and extensible. Furthermore, using regular expression for the lexical analysis prevents it from being more complex than necessary with a context-free grammar. Lastly, the lexer can be more efficient when generated from regular expressions instead of a context-free grammar~\cite{VSSD07}. 

There exists two main kinds of parsing a grammar, either top-down or bottom-up.
Top-down parsing creates a parse tree based on an input sequence of tokens starting from the root and creating the nodes in a depth-first approach. It yields a left-most derivation for the input sequence and can be implemented as a recursive-descent parser. The most common form of top-down parsing is $LL$-parsing, where the input is read from \emph{l}eft to right, yielding a \emph{l}eftmost derivation. To improve the efficiency of parsers, the context-free grammar is often restricted such that it can be parsed without backtracking with a fixed length \emph{lookahead} onto the token sequence. Such grammar are called $LL(k)$ grammars where $k$ is the length of the lookahead~\cite{VSSD07,PaFi11}.  

In contrast, bottom-up parsing builds the parse tree from the leaves up to the root. Furthermore, instead of yielding a left-most derivation, it produces a right-most derivation. Similar to top-down parsing, the most common bottom-up parsers scan the input from left to right which are therefore LR-parsers. Moreover, they can also be implemented more efficiently when restricting the grammar to a maximum lookahead. These grammar the $LR(k)$ grammars~\cite{VSSD07, PaFi11}.

An example grammar for parsing simple integer expressions is depicted in Fig.~\ref{fig:example_parser}. Similar to the regular expressions in Fig.~\ref{fig:example_lexer}, the grammar is given in ANTLR syntax. An expression is either the sum of another expression and a term or just a term. In turn, a term is either the product of a term and a factor or just a factor. Lastly, a factor is either an expression in parenthesis or an integer. Here, the definition of an integer is omitted. However, it can be seen in the previous example. The grammar is defined such that a generated parse tree inherently adheres to the order of operations.

\begin{figure}[htp]
    \centering
    \lstinputlisting[style=ANTLR]{../figures/code/background/example_parser.g4}
    \caption{An example of a context-free grammar for parsing simple expressions.}
    \label{fig:example_parser}
\end{figure}


\subsection{Semantic Analysis}
\label{sec:background_semanticalAnalysis}
% While the syntactic analysis is accomplished by context-free grammars, an analysis of the program including context is practical for finding semantic errors, \eg the use of undefined variables. Therefore, the next step in the compiler process is the semantic analysis of the parse tree.
The parser analyzes the syntactic structure of a program with a context-free grammar; however, an analysis without any context is not sufficient for an analysis of non-syntactic, \ie semantic, constraints of the program. This step is performed by the semantic analysis. The semantic analysis is used to throw semantic errors that may prevent the program from being compiled such as the use of undefined identifiers. Further, it may also enforce constraints that prevent runtime error such as type checking in a strongly typed language. Additionally, the analysis step may also process and save declarations and similar information to a symbol table which can be used in the code generation or optimization~\cite{Oliv07, SWW*88}. Moreover, the semantic analysis may not only throw errors but can also be used to infer additional information for further compilation steps. For example, besides preventing operations on operands with invalid types, the analysis may deduce which operation to apply to the operands based on their type; in the case of two integers, the analysis may infer an integer additions for the ``+''-operator while two floating point values require floating point operations~\cite{Wait74,VSSD07}.

What specifically the semantic analysis does is dependent on the design of the language being analyze. For example, a loosely typed language may have limited type checking, when compared to a strongly typed language, if any at all. Further, the implementation of the analysis can differ greatly. However, all implementations have some common elements. It requires the propagation of attributes through the syntactic structure of the program to enable the analysis. In the case of type checking, the analysis must pass on the type of a variable. Moreover, it does not only need to know the types of variables and constants, \ie leafs in a parse tree, but also the resulting type of an expression using them. For example, a integer added to a floating point value may result in a floating point value. To infer and propagate these attributes, the parse tree may need to be transverse~\cite{Wait74,VSSD07}.

\subsection{Code Generation}
\label{sec:background_codeGeneration}
After the semantic analysis of the program, which, at this stage, is in the form of a parse tree, the compiler can generate the code. Here, the compiler can either generate the target code, \eg machine code, directly or translate the parse tree into an intermediate code. 
The translation of the source code to the intermediate can be thought of as the \emph{frontend} of the compiler, with the translation of the intermediate to the target being the \emph{backend}. While the intermediate code will need to be translated again into the target language, the use of an intermediate representation can increase the modularity and extensibility of a compiler. Additionally, it can also ease the construction of a new compiler. When creating a new compiler from a source language to a target language the front end of an existing compiler for the source can be combined with an existing compiler to the target if both are using the same intermediate language~\cite{VSSD07, GFH82}.     

The most common issues when generation the target code are the evaluation order of expressions, register and storage allocation as well as related issues, context switches, and instruction selection~\cite{GFH82}. While these issues are critical for compilers that translate classical languages, \ie not quantum languages, to machine code, they are mostly not relevant for the translation of quantum computers, since quantum computers do not offer same features and abstractions that classical computer do; they have, \eg, no storage, other than the quantum registers.
Therefore, we will not discuss these issues in more detail.

\subsection{Optimization}
\label{sec:background_codeOptimization}
While the lexical, syntax, and semantic analysis combined with the code generation are the essential parts of a compiler, without which it would not work, the optimization step is also important. It used to apply either machine-independent or machine-dependent optimizations. The optimizations can be applied to the parse tree, a possible intermediate representation, and the generated target code depending on the optimization itself. While the removal of unreachable code, \eg code after a return statement, can most easily be performed on the parse tree, machine-dependent optimizations can, more appropriately, be performed on the target or intermediate code~\cite{Oliv07,VSSD07}.

Two collaborating machine-independent optimizations that are often applied by compilers are \emph{constant propagation} and \emph{constant folding}. Constant propagation analyzes the code to find variables with constant values throughout all executions and replaces the variables in, \eg, expressions with their corresponding constant value. By itself, constant propagation may only result in marginal improvement, loading a constant literal instead of the values of a variable; however, in combination with constant folding it can result significant improvement. Constant folding evaluates expressions or subexpressions with constant values at compile time, resulting in less calculations at runtime. This can significantly increase the performance of a program especially if large expressions or expressions in loops can be folded. Propagating constant values through the code enables more constant folding and, therefore, can improve its effectiveness~\cite{WeZa91}.

Another optimization technique, that can work in tandem with constant folding and propagation, is \emph{loop unrolling}. When executing a loop, each iteration needs to check the halting condition and possible execute an increment statement which can result in significant overhead. Furthermore, since the condition is checked before each iteration, the different executions cannot be executed in parallel. To prevent or reduce the performance overhead from these issues, the loop body can be executed multiple times and the increment statement adjusted accordingly. Further, if the number of iterations is constant, the loop can be removed entirely and replaced by the repeating loop body~\cite{HuLe99}. In this case, constant propagation and unrolling can help evaluate the halting condition such that the loop can be unrolled. 

Similar to loop unrolling, \emph{function inlining} also replaces some part of the code with an equivalent code body to reduce overhead. The inlining of a function replaces a function call with the function body. This mitigates the overhead cause by the function call. Additionally, the inlining also enables or simplifies further optimization such as constant folding and propagation~\cite{TGS22}. However, excessive function inlining may significantly increase the code size and, therefore, have negative effects on the caching of the code and, in turn, on the performance. This effect can be decreased or mitigated by other transformations or code reductions enabled by the inlining~\cite{PeMa02}.

Lastly, \emph{peephole optimization} is concerned with optimizing inefficient code patterns by analyzing mostly the machine or intermediate code and removing or replacing them. The pattern can be replaced with a reduced number of instructions that have the same effect or instructions that are easier to execute. For example, loading a constant value of $0$ and executing an addition has no effect and can be removed while a multiplication by $8$ can be replaced with a cheaper left shift by $3$. Furthermore, depending on the language operated on, peephole optimization can also implement some rudimentary constant folding; loading two constants $A, B$ and adding them can be replaced by loading the constant $A + B$. These patterns can be saved in tables and systematically applied to the program code~\cite{McKe65,TvS82}.
% For example, the loaded value is stored to register and the same register is immediately loaded again; in this case, the loading of the register can be removed because this value is already loaded.  

\subsection{Tools}
\label{sec:background_compilerTools}
A compiler is a complex program that can not only require a lot of coding but also is also likely to include errors when written from scratch. While the earlier syntactic stages use general algorithms, writing a custom lexer and parser can require a substantial workload and is prone to errors. Therefore, there exist many tools that can either help create a lexer and parser or generate them entirely~\cite{PaFi11, ZLY17}. In the following, we can to briefly discuss different available compiler generation tools.

Two tool for compiler generation, often used in tandem, are the Fast Lexical Analyzer (\emph{Flex})\footnote{\url{https://github.com/westes/flex}} and GNU \emph{Bison}\footnote{\url{https://github.com/akimd/bison}}~\cite{DoSt99}. 
The Flex is a lexer generator while Bison is a general-purpose parser generator. Both generators target C and C++ code with Bison having the experimental feature to support Java. 
Both tools are an extension and improvement of previous tools, \emph{Lex} and \emph{Yacc} respectively. 
Bison implements a bottom-up parser and can parse most grammars. However, the tool is optimized for $LR(1)$ grammars, \ie bottom-up parsing with a lookahead of a single token~\cite{ZLY17,Aaby03,DoSt99}. 

While Flex and Bison are separate tools for the lexing and parsing of program code, \emph{ANTLR}\footnote{\url{https://github.com/antlr/antlr4}}~\cite{PaQu95} ``ANother Tool for Language Recognition'' combines both purposes into a single tool. In contrast to Bison, it implements top-down parsing and can recognize any $LL(k)$ grammar with $k > 1$. Additionally, while Flex and Bison are mainly targeting C and C++, ANTLR can generate lexers and scanners in a variety of languages, including C++, Java, Python, and C\#.
With its newest version ANTLR4, the lexing and parsing rules can be given in the form of a context-free grammar with the terminals of the grammar given as regular expressions. At the beginning of each grammar, the name of the grammar is given and it is indicated whether the grammar describes a lexer, parser, or a general grammar, possibly containing both. The grammar rules can be either lexer or parser rules. They alway begin with a rule name, followed by a colon and the different alternatives, separated by a vertical bar, and terminated with a semicolon. While parsing rule names are given in lowercase letters, lexer rule names must begin with upper case letters.
Additionally, in a grammar rule, any symbol can be given a variable name by first giving the name followed by an equals sign ``='' and the corresponding symbol. Later, the variable name can be used when, \eg, traversing the parse tree to more easily access the reference to a symbol in the rule.

\begin{figure}[htp]
    \centering
    \lstinputlisting[style=ANTLR]{../figures/code/background/simple_exp.g4}
    \caption{Simple ANTLR4 grammar for expressions.}
    \label{fig:example_antlr4}
\end{figure}

A simple ANTLR4 grammar is depicted in Fig.~\ref{fig:example_antlr4}. Firstly, is indicates that this grammar describes a general lexer-parser combination and gives it the name \mbox{``simple\_exp''}. Then, a parsing rule called ``expression'' is defined where the expression is either just an integer or an integer, and operator and another expression. Lastly, the operator and integer lexing rules are defined. The operator is either the ``+'' or ``-'' character and the integer is a regular expression for an arbitrary sequence of number characters without a leading zero. Lastly, in the expression rule, the \texttt{OPERATOR} symbol is given the variable name \texttt{op}, which can be used to reference the operator later on.