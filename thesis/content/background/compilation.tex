\section{Compilation}
The execution on a computer is controlled by a program. This program is written in a specific language unique to the hardware of the computer, machine code. However, this language is often neither human readable nor suitable for writing complex systems. Therefore, most programs are written in a more accessible language. The program can then be translated to the machine code with a \emph{compiler}. 

A compiler translates a program written in a source language to a program in a target language. The compilation process can be divided into multiple steps. The first step ist the \emph{lexical analysis} of the source code. Next, the source codes syntactic structure is analyzed be the \emph{parser}. Then, the code is \emph{semantically analyzed} and the target code is generated in the \emph{code generation} step. Lastly, the compiler may \emph{optimize} the target code~\cite{Oliv07}. In the following, we be discuss the different steps of a compiler individually.

\subsection{Lexical Analysis}
The lexical analysis of the source program takes the character stream and groups together associated characters producing a sequence of tokens~\cite{Oliv07}. Therefore, the step is also referred to as \emph{tokenization}~\cite{Gref99}. The process can be divided into the \emph{scanning} and \emph{screening} of the program~\cite{DeRe74}.


The scanning process groups together substrings into textual elements, or tokens. In contrast to the characters and substrings, these tokens have defined meanings and may have additional attributes. For example, they may include identifiers, operator, comments, and spaces. In the case of the identifier token, an additional attribute could be the string value of the identifier.  They can be specified with the help of a regular grammar or regular expression~\cite{DeRe74,VSSD07}.\unsure{\cite{VSSD07} is extensive book, cite specific chapter somehow?}  

After being divided into a sequence of tokens, the screening step drops any characters or sequences of characters not relevant to the compilation from the program code. 
These may include characters such as spaces and tabs, or white space in general, and character sequences such as comments. 
Further, is may also recognize additional special symbols, such as keywords, and map them to a designated token~\cite{DeRe74}.\improvement{example for regular expression (simple tokens)}

\subsection{Parser}
The lexical analysis of the compiler yields a sequence of token with a known meaning the structure of the program, however, is not apparent in the token sequence. For example, an operator token does not indicate which which other tokens it operates on. To gain knowledge of the structure of the program, the parser step of the compiler analyzes the syntactic structure of the source program and creates a parse tree from it. 
\improvement{Lots of ``of''s}
The compiler can then use the tree by, \eg, walking over it to generate the target code. This step should also detect and report any syntactical errors, \eg a missing closing parentheses~\cite{VSSD07}.

The syntactic structure of a program can be represented by a context-free grammar.
\improvement{example of grammar}

\subsection{Semantic Analysis}
While the syntactic analysis is accomplished by context-free grammars, an analysis of the program including context is practical to semantic errors, \eg the use of undefined variables. \dots

\subsection{Code Generation}

\subsection{Optimization}
\label{sec:background_compiler_codeOptimization}
\begin{itemize}
    \item Different optimization techniques
    \begin{itemize}
        \item Constant folding or constant propagation
        \item Peephole optimization
    \end{itemize}
\end{itemize}

\subsection{ANTLR}
\begin{itemize}
    \item Give overview of ANTLR~\cite{PaQu95} and parsing in general
\end{itemize}